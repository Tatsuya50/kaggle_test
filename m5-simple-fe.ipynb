{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# シンプルな特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple \"Memory profilers\" to see memory usage\n",
    "# CPUのmemoryデータを見る\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Reducer\n",
    "# 省メモリ用の関数\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "# 左結合を行う関数\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "TARGET = 'sales'         # Our main target\n",
    "START_TRAIN = 29\n",
    "END_TRAIN = 1941         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "* train_dfのデータをevaluationを含めて再作成する  \n",
    "evaluationデータを加算するとメモリがオーバーしてしまうのでトレーニングデータを29日目以降にしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "########################### Load Data\n",
    "#################################################################################\n",
    "print('Load Main Data')\n",
    "\n",
    "# Here are reafing all our data \n",
    "# without any limitations and dtype modification\n",
    "train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = train_df.iloc[:, :6]\n",
    "train_df = pd.concat([train_temp, train_df.iloc[:, 5 + START_TRAIN:]], axis=1)\n",
    "del train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startとendを除いたNBA Finalsの日程\n",
    "nba_finals_dates = [\n",
    "    \"2011-06-02\", \"2011-06-05\", \"2011-06-07\",\n",
    "    \"2011-06-09\", \n",
    "    \"2012-06-14\", \"2012-06-17\", \"2012-06-19\",\n",
    "    \"2013-06-09\", \"2013-06-11\", \"2013-06-13\",\n",
    "    \"2013-06-16\", \"2013-06-18\", \n",
    "    \"2014-06-08\", \"2014-06-10\", \"2014-06-12\",\n",
    "    \"2015-06-07\", \"2015-06-09\", \"2015-06-11\",\n",
    "    \"2015-06-14\", \n",
    "    \"2016-06-05\", \"2016-06-08\", \"2016-06-10\",\n",
    "    \"2016-06-13\", \"2016-06-16\"\n",
    "]\n",
    "\n",
    "nba_index=pd.DataFrame({})\n",
    "for nba in (nba_finals_dates):\n",
    "    nba_index = nba_index.append(calendar_df[calendar_df['date']==nba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_index_l = nba_index.index\n",
    "for l in (nba_index_l):\n",
    "    if calendar_df.at[l, 'event_type_1'] == float('Nan'):\n",
    "        calendar_df.at[l, 'event_type_1'] = 'Sporting'\n",
    "        calendar_df.at[l, 'event_name_1'] = 'NBA'\n",
    "    else:\n",
    "        calendar_df.at[l, 'event_type_2'] = 'Sporting'\n",
    "        calendar_df.at[l, 'event_name_2'] = 'NBA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nba_index_l, nba_index, nba_finals_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "* event_type:SaleとしてBlack Fridayを追加した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ブラックフライデーと正月・クリスマスは特徴量を作成しておくことが推奨されている。\n",
    "# 正月・クリスマスに関しては既にevent-type-1に追加されているのでブラックフライデーを考えたい。\n",
    "# ブラックフライデー：11月の最終金曜日\n",
    "# ブラックフライデーの前日、前々日程度は特徴量として持っておく。\n",
    "black_friday = [\n",
    "    \"2011-11-25\", \"2012-11-23\", \"2013-11-29\", \"2014-11-28\", \"2015-11-27\" \n",
    "]\n",
    "black_friday_before = [\n",
    "    \"2011-11-24\", \"2012-11-22\", \"2013-11-28\", \"2014-11-27\", \"2015-11-26\"\n",
    "]\n",
    "black_friday_after = [\n",
    "    \"2011-11-26\", \"2012-11-24\", \"2013-11-30\", \"2014-11-29\", \"2015-11-28\"\n",
    "]\n",
    "\n",
    "black_index=pd.DataFrame({})\n",
    "for black in (black_friday):\n",
    "    black_index = black_index.append(calendar_df[calendar_df['date']==black])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_index_l = black_index.index\n",
    "for l in (black_index_l):\n",
    "    if calendar_df.at[l, 'event_type_1'] == float('Nan'):\n",
    "        calendar_df.at[l, 'event_type_1'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_1'] = 'Black Friday'\n",
    "    else:\n",
    "        calendar_df.at[l, 'event_type_2'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_2'] = 'Black Friday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del black_index, black_friday, black_index_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black_friday_beforeの追加\n",
    "black_index=pd.DataFrame({})\n",
    "for black in (black_friday_before):\n",
    "    black_index = black_index.append(calendar_df[calendar_df['date']==black])\n",
    "black_index_l = black_index.index\n",
    "for l in (black_index_l):\n",
    "    if calendar_df.at[l, 'event_type_1'] == float('Nan'):\n",
    "        calendar_df.at[l, 'event_type_1'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_1'] = 'Black Friday Before'\n",
    "    else:\n",
    "        calendar_df.at[l, 'event_type_2'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_2'] = 'Black Friday Before'\n",
    "del black_index, black_friday_before, black_index_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black_friday_afterの追加\n",
    "black_index=pd.DataFrame({})\n",
    "for black in (black_friday_after):\n",
    "    black_index = black_index.append(calendar_df[calendar_df['date']==black])\n",
    "black_index_l = black_index.index\n",
    "for l in (black_index_l):\n",
    "    if calendar_df.at[l, 'event_type_1'] == float('Nan'):\n",
    "        calendar_df.at[l, 'event_type_1'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_1'] = 'Black Friday Before'\n",
    "    else:\n",
    "        calendar_df.at[l, 'event_type_2'] = 'Sale'\n",
    "        calendar_df.at[l, 'event_name_2'] = 'Black Friday Before'\n",
    "del black_index, black_friday_after, black_index_l"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "* クリスマスは全店休業日となっている。トレーニングデータから除外してしまってもいいかもしれない。\n",
    "* 祝日データを追加したい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 58327370\n",
      "    Original grid_df:   3.5GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Make Grid\n",
    "#################################################################################\n",
    "print('Create Grid')\n",
    "\n",
    "# We can tranform horizontal representation \n",
    "# to vertical \"view\"\n",
    "# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n",
    "# and labels are 'd_' coulmns\n",
    "\n",
    "'''\n",
    "元データではitem_idやdept_idなどのidデータが変数名として扱われている。\n",
    "これをidデータとして扱うため、pd.meltでデータ変形を行う。\n",
    "→以下のようなデータフレームへの変形となる。\n",
    "\tid\titem_id\tdept_id\tcat_id\tstore_id\tstate_id\td\tsales\n",
    "0\tHOBBIES_1_001_CA_1_validation\tHOBBIES_1_001\tHOBBIES_1\tHOBBIES\tCA_1\tCA\td_1\t0\n",
    "1\tHOBBIES_1_002_CA_1_validation\tHOBBIES_1_002\tHOBBIES_1\tHOBBIES\tCA_1\tCA\td_1\t0\n",
    "2\tHOBBIES_1_003_CA_1_validation\tHOBBIES_1_003\tHOBBIES_1\tHOBBIES\tCA_1\tCA\td_1\t0\n",
    "3\tHOBBIES_1_004_CA_1_validation\tHOBBIES_1_004\tHOBBIES_1\tHOBBIES\tCA_1\tCA\td_1\t0\n",
    "4\tHOBBIES_1_005_CA_1_validation\tHOBBIES_1_005\tHOBBIES_1\tHOBBIES\tCA_1\tCA\td_1\t0\n",
    "\n",
    "目的変数を一意に定める説明変数の作成を行う形\n",
    "'''\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# If we look on train_df we se that \n",
    "# we don't have a lot of traning rows\n",
    "# but each day can provide more train data\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "\n",
    "# To be able to make predictions\n",
    "# we need to add \"test set\" to our grid\n",
    "# test用のデータ作成。add_gridは1914日目から28日間のデータを格納する。\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])# テスト用データの追加\n",
    "grid_df = grid_df.reset_index(drop=True)# indexを連番に振り直す\n",
    "\n",
    "# メモリ節約\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We will not need original train_df\n",
    "# anymore and can remove it\n",
    "del train_df\n",
    "\n",
    "# You don't have to use df = df construction\n",
    "# you can use inplace=True instead.\n",
    "# like this\n",
    "# grid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "# grid_dfのdtypesを単純なstrからcategoryに変換することでメモリ削減を図っている（？）\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "# メモリの食い方を確認してみるとメモリ削減関数を使用したことで約1/3程度のメモリでgrid_dfが形成されていることがわかる。\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 特徴量を増やすとしたらここらあたりから \n",
    "### まずは売上があがったタイミングをデータに反映するpart1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   1.8GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Product Release date\n",
    "#################################################################################\n",
    "print('Release week')\n",
    "\n",
    "# It seems that leadings zero values\n",
    "# in each train_df item row\n",
    "# are not real 0 sales but mean\n",
    "# absence for the item in the store\n",
    "# we can safe some memory by removing\n",
    "# such zeros\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "# 売上があがった初週を取得\n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()# 降順で並べ直す\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Now we can merge release_df\n",
    "# 売上が上がった初週のデータをgrid_dfにマージ\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# We want to remove some \"zeros\" rows\n",
    "# from grid_df \n",
    "# to do it we need wm_yr_wk column\n",
    "# let's merge partly calendar_df to have it\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows \n",
    "# and safe memory \n",
    "# 該当品目の最初の売上が上がるまでの週は不要なデータとして削除を行う。\n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Should we keep release week \n",
    "# as one of the features?\n",
    "# Only good CV can give the answer.\n",
    "# Let's minify the release values.\n",
    "# Min transformation will not help here \n",
    "# as int16 -> Integer (-32768 to 32767)\n",
    "# and our grid_df['release'].max() serves for int16\n",
    "# but we have have an idea how to transform \n",
    "# other columns in case we will need it\n",
    "# 売上初週のデータを特徴量として保持すべきかはCVによって決定すべき内容。現時点では保持しておく。\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (47397822, 10)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 1\n",
    "#################################################################################\n",
    "print('Save Part 1')\n",
    "\n",
    "# We have our BASE grid ready\n",
    "# and can save it as pickle file\n",
    "# for future use (model training)\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### part2：priceデータの反映"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "########################### Prices ここの特徴量を増やすとメモリがオーバーしてしまうのでここはデフォルトのままで進める\n",
    "#################################################################################\n",
    "print('Prices')\n",
    "\n",
    "# We can do some basic aggregations\n",
    "# 店舗・品目別のmax, min, std, meanのpricesをデータに追加する。\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "# さらにmin-max正規化を行った行の追加\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "# pricesの変動がどれだけ起きたかを行に追加する。\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "# 周期的なデータ拡張を行うため、年次、月次のデータをcalendar_dfから追加する。\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "# 年次ごと、月次ごとのprices集計を行い、この平均とその時点でのpriceを割ることでモーメンタムの作成を行う\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge prices and save part 2\n",
      "Mem. usage decreased to 1809.57 Mb (62.2% reduction)\n",
      "Size: (47397822, 13)\n"
     ]
    }
   ],
   "source": [
    "########################### Merge prices and save part 2\n",
    "#################################################################################\n",
    "print('Merge prices and save part 2')\n",
    "\n",
    "# Merge Prices\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Safe part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need prices_df anymore\n",
    "del prices_df\n",
    "\n",
    "# We can remove new columns\n",
    "# or just load part_1\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### part3：calendarデータを使用して特徴量を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Merge calendar\n",
    "#################################################################################\n",
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "# pandas.dtメソッドによって日付データを日次、週次、月次でデータ変換を行い特徴量として追加する。\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (47397822, 16)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 3 (Dates)\n",
    "#################################################################################\n",
    "print('Save part 3')\n",
    "\n",
    "# Safe part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Some additional cleaning\n",
    "#################################################################################\n",
    "\n",
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk'\n",
    "# as test values are not in train set\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n########################### Summary\\n#################################################################################\\n\\n# Now we have 3 sets of features\\n# grid_part_1~3の3種類のinputデータを結合することによってfull gridデータの作成が完了する。\\n# ただし、full girdデータをそのままトレーニングに使用するとデータが巨大すぎる。\\ngrid_df = pd.concat([pd.read_pickle(\\'grid_part_1.pkl\\'),\\n                     pd.read_pickle(\\'grid_part_2.pkl\\').iloc[:,2:],\\n                     pd.read_pickle(\\'grid_part_3.pkl\\').iloc[:,2:]],\\n                     axis=1)\\n                     \\n# Let\\'s check again memory usage\\nprint(\"{:>20}: {:>8}\".format(\\'Full Grid\\',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\\nprint(\\'Size:\\', grid_df.shape)\\n\\n# 2.5GiB + is is still too big to train our model\\n# (on kaggle with its memory limits)\\n# and we don\\'t have lag features yet\\n# But what if we can train by state_id or shop_id?\\n# full gridデータを使用するとデータが巨大すぎるので州ごと、もしくは店舗ごとにトレーニングを行うことでメモリオーバーを防げるのではないかと考えている。\\nstate_id = \\'CA\\'\\ngrid_df = grid_df[grid_df[\\'state_id\\']==state_id]\\nprint(\"{:>20}: {:>8}\".format(\\'Full Grid\\',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\\n#           Full Grid:   1.2GiB\\n\\nstore_id = \\'CA_1\\'\\ngrid_df = grid_df[grid_df[\\'store_id\\']==store_id]\\nprint(\"{:>20}: {:>8}\".format(\\'Full Grid\\',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\\n#           Full Grid: 321.2MiB\\n\\n# Seems its good enough now\\n# In other kernel we will talk about LAGS features\\n# Thank you.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "########################### Summary\n",
    "#################################################################################\n",
    "\n",
    "# Now we have 3 sets of features\n",
    "# grid_part_1~3の3種類のinputデータを結合することによってfull gridデータの作成が完了する。\n",
    "# ただし、full girdデータをそのままトレーニングに使用するとデータが巨大すぎる。\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "                     \n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# 2.5GiB + is is still too big to train our model\n",
    "# (on kaggle with its memory limits)\n",
    "# and we don't have lag features yet\n",
    "# But what if we can train by state_id or shop_id?\n",
    "# full gridデータを使用するとデータが巨大すぎるので州ごと、もしくは店舗ごとにトレーニングを行うことでメモリオーバーを防げるのではないかと考えている。\n",
    "state_id = 'CA'\n",
    "grid_df = grid_df[grid_df['state_id']==state_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid:   1.2GiB\n",
    "\n",
    "store_id = 'CA_1'\n",
    "grid_df = grid_df[grid_df['store_id']==store_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid: 321.2MiB\n",
    "\n",
    "# Seems its good enough now\n",
    "# In other kernel we will talk about LAGS features\n",
    "# Thank you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
